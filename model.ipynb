{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Code Description**:  \n",
        "1.   This code performs text classification on a dataset of comments, where the goal is to predict if a comment is toxic or not. The code imports necessary libraries, loads the data, preprocesses it, and balances the target feature. Then it splits the data into training and testing sets, vectorizes the text data using CountVectorizer and TfidfVectorizer, trains different models including Logistic Regression and Multinomial Naive Bayes, and evaluates them using classification reports. Finally, it selects the best model and evaluates it on the testing set using the F1 score.\n",
        "2.   The preprocessing step applies lowercasing, removal of stop words, and lemmatization to the comment text. The target feature imbalance is handled by downsampling the non-toxic comments to match the number of toxic comments.\n",
        "\n",
        "3. The vectorization step uses both CountVectorizer and TfidfVectorizer with n-gram range of 1 to 2 and maximum features of 50,000.\n",
        "\n",
        "4. The code trains two models, Logistic Regression and Multinomial Naive Bayes, and evaluates them using classification reports for both CountVectorizer and TfidfVectorizer. The results show that Logistic Regression with TfidfVectorizer has the best performance with an F1 score of 0.89.\n",
        "\n",
        "5. Finally, the code selects the best model (Logistic Regression with TfidfVectorizer) and evaluates it on the testing set using the classification report and F1 score.\n",
        "\n",
        "6. At last, for testing a sample comment, it applies the same preprocessing function used in the data preparation step. Then it uses the pipeline object, which represents the best trained model, to predict whether the comment is toxic or not. The predict method takes a list of comments as input, so we pass a list with only one element [sample_comment]. Finally, the code uses a conditional statement to check whether the predicted label is equal to 1 or 0, and prints the corresponding message.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZ0hX_57iA5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing necessary libraries:**"
      ],
      "metadata": {
        "id": "2s-IHVe6d27O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj3a_5V-5tH-",
        "outputId": "64f98227-88b1-488e-ccff-9314babaff26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the data**"
      ],
      "metadata": {
        "id": "uunagRypeD8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/zummit/P1 Data/train.csv\", error_bad_lines=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WfvfQ_M8PL3",
        "outputId": "e56de672-a467-4df8-dccb-ab3e8696e0d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c1cb7a0e09b4>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv(\"/content/drive/MyDrive/zummit/P1 Data/train.csv\", error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "BA3Q3UiReKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "-zOlzWe_bGnY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['comment_text'] = data['comment_text'].apply(preprocess)"
      ],
      "metadata": {
        "id": "Ndw_Z5cAbKqO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Target feature imbalance**"
      ],
      "metadata": {
        "id": "aEHIjC6ceRyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_comments = data[data['toxic']==1]\n",
        "non_toxic_comments = data[data['toxic']==0].sample(n=len(toxic_comments), random_state=42)\n",
        "balanced_data = pd.concat([toxic_comments, non_toxic_comments])"
      ],
      "metadata": {
        "id": "E8-WrcNtboOl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting data into training and testing sets**"
      ],
      "metadata": {
        "id": "p5iODrrDeZk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = balanced_data['comment_text']\n",
        "y = balanced_data['toxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xpR1k4z7bw5F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vectorization and model training**"
      ],
      "metadata": {
        "id": "GRzIozJaegk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(1,2), max_features=50000)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
        "models = [\n",
        "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
        "    ('Multinomial Naive Bayes', MultinomialNB())\n",
        "]\n",
        "for model_name, model in models:\n",
        "    pipeline_cv = Pipeline([('cv', cv), ('model', model)])\n",
        "    pipeline_tfidf = Pipeline([('tfidf', tfidf), ('model', model)])\n",
        "    pipeline_cv.fit(X_train, y_train)\n",
        "    pipeline_tfidf.fit(X_train, y_train)\n",
        "    y_pred_cv = pipeline_cv.predict(X_test)\n",
        "    y_pred_tfidf = pipeline_tfidf.predict(X_test)\n",
        "    print(model_name + \" with CountVectorizer\\n\" + classification_report(y_test, y_pred_cv))\n",
        "    print(model_name + \" with TfidfVectorizer\\n\" + classification_report(y_test, y_pred_tfidf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVEoNEpNbyx-",
        "outputId": "3cc49930-98c0-46b0-e6fd-59e735e85755"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      2997\n",
            "           1       0.89      0.89      0.89      3121\n",
            "\n",
            "    accuracy                           0.89      6118\n",
            "   macro avg       0.89      0.89      0.89      6118\n",
            "weighted avg       0.89      0.89      0.89      6118\n",
            "\n",
            "Logistic Regression with TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.90      2997\n",
            "           1       0.92      0.87      0.89      3121\n",
            "\n",
            "    accuracy                           0.89      6118\n",
            "   macro avg       0.90      0.89      0.89      6118\n",
            "weighted avg       0.90      0.89      0.89      6118\n",
            "\n",
            "Multinomial Naive Bayes with CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      2997\n",
            "           1       0.90      0.87      0.88      3121\n",
            "\n",
            "    accuracy                           0.88      6118\n",
            "   macro avg       0.88      0.88      0.88      6118\n",
            "weighted avg       0.88      0.88      0.88      6118\n",
            "\n",
            "Multinomial Naive Bayes with TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88      2997\n",
            "           1       0.89      0.87      0.88      3121\n",
            "\n",
            "    accuracy                           0.88      6118\n",
            "   macro avg       0.88      0.88      0.88      6118\n",
            "weighted avg       0.88      0.88      0.88      6118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing the best model**"
      ],
      "metadata": {
        "id": "D-8Qyd0CekXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([('tfidf', tfidf), ('model', LogisticRegression(random_state=42))])\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"Logistic Regression with TfidfVectorizer\\n\" + classification_report(y_test, y_pred))\n",
        "print(\"F1-score: {:.2f}\".format(f1_score(y_test, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8fBtqtSb41L",
        "outputId": "f8338778-6eb6-4c6d-d0a2-2eb644b962e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.90      2997\n",
            "           1       0.92      0.87      0.89      3121\n",
            "\n",
            "    accuracy                           0.89      6118\n",
            "   macro avg       0.90      0.89      0.89      6118\n",
            "weighted avg       0.90      0.89      0.89      6118\n",
            "\n",
            "F1-score: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing the model on a sample comment**"
      ],
      "metadata": {
        "id": "QvKS7g9KerBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_comment = \"Sample Comment\"\n",
        "sample_comment = preprocess(sample_comment)\n",
        "y_pred = pipeline.predict([sample_comment])\n",
        "y_pred = pipeline.predict([sample_comment])\n",
        "if y_pred[0] == 1:\n",
        "    print(\"The comment is toxic.\")\n",
        "else:\n",
        "    print(\"The comment is not toxic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt30DFjqct6C",
        "outputId": "2657149d-ab5a-4ab8-b967-40f26a6f88e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The comment is not toxic.\n"
          ]
        }
      ]
    }
  ]
}