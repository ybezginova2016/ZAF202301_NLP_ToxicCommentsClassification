{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing Required Libraries**"
      ],
      "metadata": {
        "id": "4IF14fKcrs0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZVkJMuPN2Lp"
      },
      "outputs": [],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "XrQFRj9cm3g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y212nTCUNFDx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "csQZUnwh-I5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load data from CSV file**"
      ],
      "metadata": {
        "id": "4LCdtO1hr2az"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14m0nPI4NKWa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNYmGw4_aTgF"
      },
      "outputs": [],
      "source": [
        "df.dropna(how='any', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_count = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()\n",
        "print(toxic_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjg1MA0zyieA",
        "outputId": "6b38eb30-d211-416b-a0fd-ff8c10bbada1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic            15294\n",
            "severe_toxic      1595\n",
            "obscene           8449\n",
            "threat             478\n",
            "insult            7877\n",
            "identity_hate     1405\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column called 'is_toxic' that indicates whether a comment is toxic or not\n",
        "df['is_toxic'] = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].any(axis=1).astype(int)\n",
        "\n",
        "# Create a new DataFrame to store the balanced dataset\n",
        "balanced_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "# Sample 5000 comments from each class\n",
        "for label in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
        "    tmp = df[df[label] == 1].sample(n=5000, replace=True)\n",
        "    balanced_df = balanced_df.append(tmp, ignore_index=True)\n",
        "\n",
        "# Drop the columns 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
        "balanced_df = balanced_df.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "# Print the first few rows of the balanced dataset\n",
        "print(balanced_df.head())\n"
      ],
      "metadata": {
        "id": "HTVCW9kbiw6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.shape[0]"
      ],
      "metadata": {
        "id": "IAzsOZKFjEtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data pre-processing**"
      ],
      "metadata": {
        "id": "F-Opyc_xr8uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoMIHBUnNMgh"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRZjdah9NOPx"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    \n",
        "    # Remove stop words and lemmatize\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    \n",
        "    # Join words back into text\n",
        "    clean_text = ' '.join(words)\n",
        "    \n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OFyrW8ENQcJ"
      },
      "outputs": [],
      "source": [
        "df['comment_text'] = df['comment_text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encode labels as integers**"
      ],
      "metadata": {
        "id": "Dl2NGMm8sKnT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB0brXotNSg4"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['is_toxic'] = label_encoder.fit_transform(df['is_toxic'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split data into training and testing sets**"
      ],
      "metadata": {
        "id": "rkXBRS1KsVMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJsS0e_qNUXQ"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['is_toxic'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQyyJGZLfdMt"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load BERT tokenizer**"
      ],
      "metadata": {
        "id": "h3M81Er2sZFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_KHvshzNWL5"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenize text and add special tokens**"
      ],
      "metadata": {
        "id": "Lk3EN4bZsdPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCEXzlu0NZeZ"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(list(train_df['comment_text']), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_df['comment_text']), truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert data to TensorFlow datasets**"
      ],
      "metadata": {
        "id": "3TAkIlSAsg1B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX1EzMoGNbor"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_df['is_toxic'].values\n",
        ")).shuffle(len(train_df)).batch(4)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_df['is_toxic'].values\n",
        ")).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build BERT model**"
      ],
      "metadata": {
        "id": "VK5OGbGZskFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwfOCjjENeWh"
      },
      "outputs": [],
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "L_4nbdI4E0Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compile model**"
      ],
      "metadata": {
        "id": "jJGPWQs4soAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwbILzCsNgjE"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=2e-5, epsilon=1e-8)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train model**"
      ],
      "metadata": {
        "id": "-EiSUm7wstcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kytn_W72Ni4y"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=1, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluate model**"
      ],
      "metadata": {
        "id": "0sP214NHsyi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROrTG8FNmZy"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_dataset)\n",
        "test_predictions = np.argmax(test_predictions.logits, axis=-1)\n",
        "test_labels = test_df['toxic'].values\n",
        "test_f1 = f1_score(test_labels, test_predictions)\n",
        "\n",
        "print('Test F1 score:', test_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving Model**"
      ],
      "metadata": {
        "id": "8VSK_qW1s-xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the best model to disk\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "oLSHAl0qtByb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing the Output**"
      ],
      "metadata": {
        "id": "UjyDMfnBw5M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_comment = \"You fool\"\n",
        "sample_comment = preprocess(sample_comment)\n",
        "pred = model.predict([sample_comment])\n",
        "if pred[0] == 1:\n",
        "    print(\"The comment is toxic.\")\n",
        "else:\n",
        "    print(\"The comment is not toxic.\")"
      ],
      "metadata": {
        "id": "C34Tif-sw5cn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4IF14fKcrs0T",
        "4LCdtO1hr2az",
        "F-Opyc_xr8uk",
        "h3M81Er2sZFH",
        "VK5OGbGZskFQ",
        "0sP214NHsyi0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}